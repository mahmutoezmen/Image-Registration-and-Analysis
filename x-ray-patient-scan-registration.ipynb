{"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\nSimpleITK, ITK, scipy, OpenCV, Tensorflow and PyTorch all offer tools for registering images, we explore a few here to see how well they work when applied to the fairly tricky problem of registering from the same person at different time and disease points.","metadata":{"_cell_guid":"19ff9d0b-d32d-4325-a19f-2ccbfa7d2dff","_uuid":"9206c5863724e1af01281071e0f0ee5109ef431b"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport tensorflow as tf # for tensorflow based registration\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch # for pytorch-based registration\nimport os\nfrom cv2 import imread, createCLAHE # read and equalize images\nimport SimpleITK as sitk\nimport cv2\nfrom glob import glob\n%matplotlib inline\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"f42f6560-edf0-4efb-85a6-6e945e50895b","_uuid":"3300a1edbf2e8122d88093998eb503a6fab8a719","execution":{"iopub.status.busy":"2022-01-12T10:03:34.997923Z","iopub.execute_input":"2022-01-12T10:03:34.998344Z","iopub.status.idle":"2022-01-12T10:03:38.443930Z","shell.execute_reply.started":"2022-01-12T10:03:34.998261Z","shell.execute_reply":"2022-01-12T10:03:38.443102Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"all_xray_df = pd.read_csv('../input/Data_Entry_2017.csv')\nall_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..', 'input', 'images*', '*', '*.png'))}\nprint('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\nall_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\nall_xray_df.sample(3)","metadata":{"_cell_guid":"9a342cdc-0823-490d-9a3a-a53fb7c33727","_uuid":"fe804e7c294e2d290e27b037bf1ba56177abab70","execution":{"iopub.status.busy":"2022-01-12T10:03:46.092470Z","iopub.execute_input":"2022-01-12T10:03:46.092823Z","iopub.status.idle":"2022-01-12T10:03:52.041170Z","shell.execute_reply.started":"2022-01-12T10:03:46.092739Z","shell.execute_reply":"2022-01-12T10:03:52.040479Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"all_xray_df[['Follow-up #', 'OriginalImagePixelSpacing[x', 'Patient Age', 'OriginalImage[Width']].hist(figsize = (10, 10))","metadata":{"_cell_guid":"50738884-22d8-4a30-885f-2be6c521deb5","_uuid":"e8c1e79f754863425e9d3ecbba3837516042dbee","execution":{"iopub.status.busy":"2022-01-12T10:04:00.401488Z","iopub.execute_input":"2022-01-12T10:04:00.402163Z","iopub.status.idle":"2022-01-12T10:04:00.948453Z","shell.execute_reply.started":"2022-01-12T10:04:00.402090Z","shell.execute_reply":"2022-01-12T10:04:00.947728Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"all_xray_df[['View Position', 'Patient Gender']].describe().T","metadata":{"_cell_guid":"07d74803-97dd-4d96-8d1d-c616e9f23a8a","_uuid":"5a47e81922a59f8dc5dbf26996bc05fb8620e466","execution":{"iopub.status.busy":"2022-01-12T10:04:39.617548Z","iopub.execute_input":"2022-01-12T10:04:39.617876Z","iopub.status.idle":"2022-01-12T10:04:39.754308Z","shell.execute_reply.started":"2022-01-12T10:04:39.617836Z","shell.execute_reply":"2022-01-12T10:04:39.753409Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Select Frequent Flyers","metadata":{"_cell_guid":"737ba6cc-f3d8-4787-87ce-0d0085e4c672","_uuid":"1e6a7491ba14744a61fafb746ee12e128bc10b81"}},{"cell_type":"code","source":"pa_xray_df = all_xray_df[all_xray_df['View Position'].isin(['PA'])].copy()\npa_xray_df = pa_xray_df[pa_xray_df['Patient Age']<120]\ntop_patients_df = pa_xray_df.groupby(['Patient ID']\n                                    ).count()[['Image Index']].reset_index().sort_values('Image Index', ascending = False).head(100)\npa_xray_df = pa_xray_df[pa_xray_df['Patient ID'].isin(top_patients_df['Patient ID'].values)].sort_values(['Patient ID', 'Follow-up #'])\nfig, ax1 = plt.subplots(1,1, figsize = (20, 20))\nfor p_id, c_rows in pa_xray_df.groupby('Patient ID'):\n    ax1.plot(c_rows['Follow-up #'], c_rows['Patient Age'], '.-', label = p_id)\nax1.legend()\nax1.set_xlabel('Follow-up #')\nax1.set_ylabel('Patient Age')\npa_xray_df.head(10)\n","metadata":{"_cell_guid":"8e0abf66-0f34-4289-8eb6-df6ddd3c2f11","_uuid":"90d2424684780ec518c8ef37f7f988cc24f4ad08","execution":{"iopub.status.busy":"2022-01-12T10:04:44.700800Z","iopub.execute_input":"2022-01-12T10:04:44.701141Z","iopub.status.idle":"2022-01-12T10:04:46.336940Z","shell.execute_reply.started":"2022-01-12T10:04:44.701062Z","shell.execute_reply":"2022-01-12T10:04:46.336083Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from skimage.transform import resize\nOUT_DIM = (512, 512)\ndef simple_imread(im_path, apply_clahe = False):\n    img_data = np.mean(imread(im_path), 2).astype(np.uint8)\n    n_img = (255*resize(img_data, OUT_DIM, mode = 'constant')).clip(0,255).astype(np.uint8)\n    if apply_clahe:\n        clahe_tool = createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        return clahe_tool.apply(n_img)\n    else:\n        return n_img","metadata":{"_cell_guid":"a1367d63-4c7b-4e47-b19f-9a26d1f89476","scrolled":true,"_uuid":"6ed6489bbd618fb419ceca2bbd8c300694f1d4ed","execution":{"iopub.status.busy":"2022-01-12T10:04:54.806381Z","iopub.execute_input":"2022-01-12T10:04:54.806701Z","iopub.status.idle":"2022-01-12T10:04:55.363477Z","shell.execute_reply.started":"2022-01-12T10:04:54.806640Z","shell.execute_reply":"2022-01-12T10:04:55.362744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_img = simple_imread(all_xray_df['path'].values[0])\nplt.matshow(test_img, cmap = 'bone')","metadata":{"_cell_guid":"c48aa60d-74c3-4664-94f8-c2ce913562a0","_uuid":"0c09a79ddfb6cb7b286f56839679a022673a2691","execution":{"iopub.status.busy":"2022-01-12T10:05:00.128199Z","iopub.execute_input":"2022-01-12T10:05:00.128502Z","iopub.status.idle":"2022-01-12T10:05:00.445651Z","shell.execute_reply.started":"2022-01-12T10:05:00.128456Z","shell.execute_reply":"2022-01-12T10:05:00.444819Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# grab the scans from the first patient\nfirst_patient_df = pa_xray_df[pa_xray_df['Patient ID'].isin([pa_xray_df['Patient ID'].values[0]])]\nprint(first_patient_df.shape[0], 'scans found')\nfirst_patient_df.head(10)","metadata":{"_cell_guid":"da9cb5c7-90f3-4818-98e0-f6a60f353d0c","_uuid":"6fc0754cc768a31005cef36b6fb3c1262fa08a19","execution":{"iopub.status.busy":"2022-01-12T10:05:03.310470Z","iopub.execute_input":"2022-01-12T10:05:03.310775Z","iopub.status.idle":"2022-01-12T10:05:03.339239Z","shell.execute_reply.started":"2022-01-12T10:05:03.310714Z","shell.execute_reply":"2022-01-12T10:05:03.338303Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\nfirst_scans = np.stack(first_patient_df['path'].map(simple_imread).values,0)","metadata":{"_cell_guid":"73a8dba1-3d8e-47ec-b945-3101ad57dcef","_uuid":"0878962d872569c57b0f959751c522ae928f92e3","execution":{"iopub.status.busy":"2022-01-12T10:05:04.925181Z","iopub.execute_input":"2022-01-12T10:05:04.925473Z","iopub.status.idle":"2022-01-12T10:05:07.267921Z","shell.execute_reply.started":"2022-01-12T10:05:04.925437Z","shell.execute_reply":"2022-01-12T10:05:07.266963Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# might as well show-em if we have em\nfrom skimage.util.montage import montage2d\nfig, ax1 = plt.subplots(1,1, figsize = (12,12))\nax1.imshow(montage2d(first_scans), cmap = 'bone')\nfig.savefig('overview.png', dpi = 300)","metadata":{"_cell_guid":"cc076f02-0bf2-4d3f-a6ab-f192bed9e6a9","_uuid":"d7168d90928d842c295c64e2446e84eb068fd391","execution":{"iopub.status.busy":"2022-01-12T10:05:34.211925Z","iopub.execute_input":"2022-01-12T10:05:34.212552Z","iopub.status.idle":"2022-01-12T10:05:39.665737Z","shell.execute_reply.started":"2022-01-12T10:05:34.212501Z","shell.execute_reply":"2022-01-12T10:05:39.664852Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Registration Problem\nWe would like to register the two images so we can focus on the differences. In the example below we try to subtract the two scans and the difference image has lots of structures from the original image visible not just the differences","metadata":{"_cell_guid":"e94f23fb-5824-45f3-b7d6-cf3892bb06ac","_uuid":"bce7abb87ce1f169b8b754298fc24da9907993a1"}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 4))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(first_scans[2], cmap = 'bone', vmax = 255)\nax2.set_title('Scan #2')\nax3.imshow(1.0*first_scans[1]-first_scans[0], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference');","metadata":{"_cell_guid":"31990e46-1437-4945-aaa5-b31b377d6538","_uuid":"a5dd26025391067cd0220b7502b193e5e095edbd","execution":{"iopub.status.busy":"2022-01-12T10:05:53.080451Z","iopub.execute_input":"2022-01-12T10:05:53.080736Z","iopub.status.idle":"2022-01-12T10:05:53.511579Z","shell.execute_reply.started":"2022-01-12T10:05:53.080699Z","shell.execute_reply":"2022-01-12T10:05:53.510891Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# OpenCV Approach\nThe easiest and oldest method is the OpenCV-based approach to find similar points on the images and match them up. A more detailed tutorial is available here: http://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/","metadata":{"_cell_guid":"12a479ac-9704-461e-b80e-0f7011faa75b","_uuid":"166e4b886114ee702a57380d4fab7e1d05c1328e"}},{"cell_type":"code","source":"import cv2\nMAX_FEATURES = 1000\nGOOD_MATCH_PERCENT = 0.20\ndef alignImages(im1Gray, im2Gray):\n    # Detect ORB features and compute descriptors.\n    orb = cv2.ORB_create(MAX_FEATURES)\n    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n    # Match features.\n    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n    matches = matcher.match(descriptors1, descriptors2, None)\n    # Sort matches by score\n    matches.sort(key=lambda x: x.distance, reverse=False)\n    # Remove not so good matches\n    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n    matches = matches[:numGoodMatches]\n    # Draw top matches\n    imMatches = cv2.drawMatches(im1Gray, keypoints1, im2Gray, keypoints2, matches, None)\n    fig, ax1 = plt.subplots(1,1, figsize = (8,8))\n    ax1.imshow(imMatches)\n    # Extract location of good matches\n    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n    for i, match in enumerate(matches):\n        points1[i, :] = keypoints1[match.queryIdx].pt\n        points2[i, :] = keypoints2[match.trainIdx].pt\n    # Find homography\n    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n    # Use homography\n    height, width = im2Gray.shape\n    im1Reg = cv2.warpPerspective(im1Gray, h, (width, height))\n    return im1Reg, h","metadata":{"_cell_guid":"abe9603d-37a9-4b90-a188-36d22c0a6112","_uuid":"bdd391d2eb3a5449375bfd34b8c15c0781ca433d","execution":{"iopub.status.busy":"2022-01-12T10:06:18.379505Z","iopub.execute_input":"2022-01-12T10:06:18.379819Z","iopub.status.idle":"2022-01-12T10:06:18.446528Z","shell.execute_reply.started":"2022-01-12T10:06:18.379780Z","shell.execute_reply":"2022-01-12T10:06:18.445705Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\nregistered_scan_2, _ = alignImages(first_scans[2], first_scans[1])","metadata":{"_cell_guid":"cf630180-22a8-4acf-9591-1fe69b4f1af1","_uuid":"97a9f331729f336217762b45a4a993d3553c7ee6","execution":{"iopub.status.busy":"2022-01-12T10:06:28.812652Z","iopub.execute_input":"2022-01-12T10:06:28.813093Z","iopub.status.idle":"2022-01-12T10:06:29.104197Z","shell.execute_reply.started":"2022-01-12T10:06:28.813039Z","shell.execute_reply":"2022-01-12T10:06:29.103356Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax1a, ax4, ax5)) = plt.subplots(2, 3, figsize = (12, 8))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(first_scans[2], cmap = 'bone', vmax = 255)\nax2.set_title('Scan #2')\nax3.imshow(1.0*first_scans[2]-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference')\nax1a.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1a.set_title('Scan #1')\nax4.imshow(registered_scan_2, cmap = 'bone', vmax = 255)\nax4.set_title('Registered Scan 2')\nax5.imshow(1.0*registered_scan_2-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax5.set_title('Post Registration Difference');","metadata":{"_cell_guid":"73184f84-ab87-40ae-b3c7-85837cf05efe","_uuid":"c70abbbd3faf64c370da1aa02bdee68e1f29c2bc","execution":{"iopub.status.busy":"2022-01-12T10:06:34.016372Z","iopub.execute_input":"2022-01-12T10:06:34.016942Z","iopub.status.idle":"2022-01-12T10:06:34.865953Z","shell.execute_reply.started":"2022-01-12T10:06:34.016883Z","shell.execute_reply":"2022-01-12T10:06:34.865082Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# SimpleITK Approach\nHere we use the tools of SimpleITK to register the two chest x-ray scans.","metadata":{"_cell_guid":"221f39db-5ca1-446e-800e-f6288e4c5d30","_uuid":"f7b679492c9b94477edc37ae6acbb00088ce12f5"}},{"cell_type":"markdown","source":"Affine Registration","metadata":{}},{"cell_type":"code","source":"def register_img(fixed_arr, \n                 moving_arr,\n                use_affine = True,\n                use_mse = True,\n                brute_force = True,\n                show_transform = True):\n    fixed_image = sitk.GetImageFromArray(fixed_arr)\n    moving_image = sitk.GetImageFromArray(moving_arr)\n    transform = sitk.AffineTransform(2) if use_affine else sitk.ScaleTransform(2)\n    initial_transform = sitk.CenteredTransformInitializer(sitk.Cast(fixed_image,moving_image.GetPixelID()), \n                                                      moving_image, \n                                                      transform, \n                                                      sitk.CenteredTransformInitializerFilter.GEOMETRY)\n    ff_img = sitk.Cast(fixed_image, sitk.sitkFloat32) # fixed image type\n    mv_img = sitk.Cast(moving_image, sitk.sitkFloat32) # moving/reference image type\n    registration_method = sitk.ImageRegistrationMethod()\n    if use_mse:\n        registration_method.SetMetricAsMeanSquares() # Similarity Metric\n    else:\n        registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=32) #Similarity Metric and number of histogram bins(for MI only)\n    \n    if brute_force:\n        sample_per_axis = 12\n        registration_method.SetOptimizerAsExhaustive([sample_per_axis//2,0,0])\n        # Utilize the scale to set the step size for each dimension\n        registration_method.SetOptimizerScales([2.0*3.14/sample_per_axis, 1.0,1.0])\n    else:\n        registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n        registration_method.SetMetricSamplingPercentage(0.25)\n\n    registration_method.SetInterpolator(sitk.sitkLinear)\n\n    registration_method.SetOptimizerAsGradientDescent(learningRate=1.0, \n                                                      numberOfIterations=4000, \n                                                      convergenceMinimumValue=1e-6,\n                                                      convergenceWindowSize=10)\n    # Scale the step size differently for each parameter, this is critical!!!\n    registration_method.SetOptimizerScalesFromPhysicalShift() \n\n    registration_method.SetInitialTransform(initial_transform, inPlace=False)\n    final_transform_v1 = registration_method.Execute(ff_img, \n                                                     mv_img)\n    print('Optimizer\\'s stopping condition, {0}'.format(registration_method.GetOptimizerStopConditionDescription()))\n    print('Final metric value: {0}'.format(registration_method.GetMetricValue()))\n    resample = sitk.ResampleImageFilter()\n    resample.SetReferenceImage(fixed_image)\n    \n    # SimpleITK supports several interpolation options, we go with the simplest that gives reasonable results.     \n    resample.SetInterpolator(sitk.sitkBSpline)  \n    resample.SetTransform(final_transform_v1)\n    if show_transform:\n        fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 6))\n        xx, yy = np.meshgrid(range(moving_arr.shape[0]), range(moving_arr.shape[1]))\n        test_pattern = (((xx % 40)>30)|((yy % 40)>30)).astype(np.float32)\n        ax1.imshow(test_pattern, cmap = 'bone_r')\n        ax1.set_title('Test Pattern')\n        test_pattern_img = sitk.GetImageFromArray(test_pattern)\n        skew_pattern = sitk.GetArrayFromImage(resample.Execute(test_pattern_img))\n        ax2.imshow(skew_pattern, cmap = 'bone_r')\n        ax2.set_title('Registered Pattern')\n    return sitk.GetArrayFromImage(resample.Execute(moving_image))","metadata":{"_cell_guid":"41be87ac-0831-4093-a4ce-e4a8df93e0a3","_uuid":"4bd67fb8bc2bb7b0482875b8ccb912161e5cabd5","execution":{"iopub.status.busy":"2022-01-12T10:11:59.745910Z","iopub.execute_input":"2022-01-12T10:11:59.746224Z","iopub.status.idle":"2022-01-12T10:11:59.907146Z","shell.execute_reply.started":"2022-01-12T10:11:59.746176Z","shell.execute_reply":"2022-01-12T10:11:59.906313Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"%%time\nregistered_scan_2 = register_img(first_scans[1], first_scans[2])","metadata":{"_cell_guid":"47950042-9fbd-41f0-88d4-0de29b32dd13","_uuid":"735d06f084a2e50fd9045a2245d342f1a2fa92ed","execution":{"iopub.status.busy":"2022-01-12T10:12:01.135488Z","iopub.execute_input":"2022-01-12T10:12:01.136116Z","iopub.status.idle":"2022-01-12T10:12:15.347750Z","shell.execute_reply.started":"2022-01-12T10:12:01.136048Z","shell.execute_reply":"2022-01-12T10:12:15.347088Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax1a, ax4, ax5)) = plt.subplots(2, 3, figsize = (12, 8))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(first_scans[2], cmap = 'bone', vmax = 255)\nax2.set_title('Scan #2')\nax3.imshow(1.0*first_scans[2]-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference')\nax1a.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1a.set_title('Scan #1')\nax4.imshow(registered_scan_2, cmap = 'bone', vmax = 255)\nax4.set_title('Deformed Fixed Image Scan 2')\nax5.imshow(1.0*registered_scan_2-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax5.set_title('Post Registration Difference');","metadata":{"_cell_guid":"3c9583a9-c7f5-421e-9cf7-5350e58e4f51","_uuid":"b30b438ddd3b9b9bb89fcc4507e216318cf2c233","execution":{"iopub.status.busy":"2022-01-12T10:12:41.853596Z","iopub.execute_input":"2022-01-12T10:12:41.853980Z","iopub.status.idle":"2022-01-12T10:12:42.855704Z","shell.execute_reply.started":"2022-01-12T10:12:41.853917Z","shell.execute_reply":"2022-01-12T10:12:42.854905Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# SimpleITK Deformable Approach\nWe can apply deformable approaches to the same problem using some of the example code given in http://insightsoftwareconsortium.github.io/SimpleITK-Notebooks/Python_html/65_Registration_FFD.html","metadata":{"_cell_guid":"abe1c6fc-c00d-4b4b-8149-d87a9ce16985","_uuid":"f0df7b9360a84575c2322bea244c27c5d0a27f1d"}},{"cell_type":"code","source":"def register_img_generic(fixed_arr, \n                 moving_arr,\n                registration_func,\n                        show_transform = True):\n    fixed_image = sitk.GetImageFromArray(fixed_arr)\n    moving_image = sitk.GetImageFromArray(moving_arr)\n    ff_img = sitk.Cast(fixed_image, sitk.sitkFloat32)\n    mv_img = sitk.Cast(moving_image, sitk.sitkFloat32)\n    \n    final_transform_v1 = registration_func(ff_img, mv_img)\n    resample = sitk.ResampleImageFilter()\n    resample.SetReferenceImage(fixed_image)\n    \n    # SimpleITK supports several interpolation options, we go with the simplest that gives reasonable results.     \n    resample.SetInterpolator(sitk.sitkBSpline)  \n    resample.SetTransform(final_transform_v1)\n    if show_transform:\n        fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 6))\n        xx, yy = np.meshgrid(range(moving_arr.shape[0]), range(moving_arr.shape[1]))\n        test_pattern = (((xx % 40)>30)|((yy % 40)>30)).astype(np.float32)\n        ax1.imshow(test_pattern, cmap = 'bone_r')\n        ax1.set_title('Test Pattern')\n        test_pattern_img = sitk.GetImageFromArray(test_pattern)\n        skew_pattern = sitk.GetArrayFromImage(resample.Execute(test_pattern_img))\n        ax2.imshow(skew_pattern, cmap = 'bone_r')\n        ax2.set_title('Registered Pattern')\n    return sitk.GetArrayFromImage(resample.Execute(moving_image))\n\ndef bspline_intra_modal_registration(fixed_image, moving_image, grid_physical_spacing =  [15.0]*3):\n    registration_method = sitk.ImageRegistrationMethod()\n    # Determine the number of BSpline control points using the physical spacing we want for the control grid. \n    image_physical_size = [size*spacing for size,spacing in zip(fixed_image.GetSize(), fixed_image.GetSpacing())]\n    mesh_size = [int(image_size/grid_spacing + 0.5) \\\n                 for image_size,grid_spacing in zip(image_physical_size,grid_physical_spacing)]\n    print('Using Mesh Size', mesh_size)\n    initial_transform = sitk.BSplineTransformInitializer(image1 = fixed_image, \n                                                         transformDomainMeshSize = mesh_size, order=3)    \n    registration_method.SetInitialTransform(initial_transform)\n        \n    registration_method.SetMetricAsMeanSquares()\n    # Settings for metric sampling, usage of a mask is optional. When given a mask the sample points will be \n    # generated inside that region. Also, this implicitly speeds things up as the mask is smaller than the\n    # whole image.\n    registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n    registration_method.SetMetricSamplingPercentage(0.01)\n    \n    # Multi-resolution framework.            \n    registration_method.SetShrinkFactorsPerLevel(shrinkFactors = [4,2,1])\n    registration_method.SetSmoothingSigmasPerLevel(smoothingSigmas=[2,1,0])\n    registration_method.SmoothingSigmasAreSpecifiedInPhysicalUnitsOn()\n\n    registration_method.SetInterpolator(sitk.sitkLinear)\n    registration_method.SetOptimizerAsLBFGSB(gradientConvergenceTolerance=1e-5, numberOfIterations=100)\n    return registration_method.Execute(fixed_image, moving_image)","metadata":{"_cell_guid":"e4956b3e-46a1-4d11-a54d-c45c67019663","_uuid":"1a9147f6423786acd69881b92781ec7274430b5b","execution":{"iopub.status.busy":"2022-01-12T10:14:00.380571Z","iopub.execute_input":"2022-01-12T10:14:00.380912Z","iopub.status.idle":"2022-01-12T10:14:00.539735Z","shell.execute_reply.started":"2022-01-12T10:14:00.380865Z","shell.execute_reply":"2022-01-12T10:14:00.538789Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"%%time\ndregistered_scan_2 = register_img_generic(first_scans[1], registered_scan_2, registration_func=bspline_intra_modal_registration)","metadata":{"_cell_guid":"35608f63-bd89-4a73-9f07-ea2eb8ef07bc","_uuid":"327a70e3d4e41eadabd64814555c31d84c57e19c","execution":{"iopub.status.busy":"2022-01-12T10:14:06.822974Z","iopub.execute_input":"2022-01-12T10:14:06.823543Z","iopub.status.idle":"2022-01-12T10:14:11.940418Z","shell.execute_reply.started":"2022-01-12T10:14:06.823222Z","shell.execute_reply":"2022-01-12T10:14:11.939593Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax1a, ax4, ax5)) = plt.subplots(2, 3, figsize = (12, 8))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(registered_scan_2, cmap = 'bone', vmax = 255)\nax2.set_title('Registered Scan #2')\nax3.imshow(1.0*registered_scan_2-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference')\nax1a.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1a.set_title('Scan #1')\nax4.imshow(dregistered_scan_2, cmap = 'bone', vmax = 255)\nax4.set_title('Deformably Registered Scan 2')\nax5.imshow(1.0*dregistered_scan_2-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax5.set_title('Post Registration Difference');","metadata":{"_cell_guid":"cd5e5650-879b-4437-926b-8c6ab024a04f","_uuid":"ef53fc364f955e6cc30051a10a89fa19937be632","execution":{"iopub.status.busy":"2022-01-12T10:14:18.843922Z","iopub.execute_input":"2022-01-12T10:14:18.844452Z","iopub.status.idle":"2022-01-12T10:14:19.693464Z","shell.execute_reply.started":"2022-01-12T10:14:18.844392Z","shell.execute_reply":"2022-01-12T10:14:19.692576Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Approach\nPyTorch is well suited to these kind of optimization problems since like Tensorflow it is designed for performant computations on tensors and has predefined operations to handle gradient descent, spatial transformations, and other important functions. We use the [Spatial Transformer tutorial](https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html) by Ghassen Hamrouni as a baseline for the example here. ","metadata":{"_uuid":"7f9ea12a9c761bfb7bff55e8b959c0b41a56777a"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"940ca997e3bfa2c461f146f71bd92131bf9e6a00","execution":{"iopub.status.busy":"2022-01-12T10:14:27.322419Z","iopub.execute_input":"2022-01-12T10:14:27.322713Z","iopub.status.idle":"2022-01-12T10:14:27.330610Z","shell.execute_reply.started":"2022-01-12T10:14:27.322657Z","shell.execute_reply":"2022-01-12T10:14:27.329675Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class SimpleAffineRegistration(nn.Module):\n    def __init__(self):\n        super(SimpleAffineRegistration, self).__init__()\n        self.theta = Parameter(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float).view(2,3))\n    # Spatial transformer network forward function\n    def stn(self, x, theta):\n        theta_vec = theta.repeat((x.shape[0], 1, 1))\n        grid = F.affine_grid(theta_vec, x.size())\n        return F.grid_sample(x, grid)\n\n    def forward(self, x):\n        # transform the input\n        return self.stn(x, self.theta)\n\nmodel = SimpleAffineRegistration().to(device)","metadata":{"_uuid":"2e334affaef0b2e79d1ca8252ad06a88c10821b8","execution":{"iopub.status.busy":"2022-01-12T10:14:28.259238Z","iopub.execute_input":"2022-01-12T10:14:28.259558Z","iopub.status.idle":"2022-01-12T10:14:28.294070Z","shell.execute_reply.started":"2022-01-12T10:14:28.259487Z","shell.execute_reply":"2022-01-12T10:14:28.293119Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Setup the Tensors\nHere we setup the tensors and show the model appears to work for the basic no deformation case**","metadata":{"_uuid":"745703ef24d9ad5e31a6da7dc0b5a39f8b19703b"}},{"cell_type":"code","source":"moving_tensor = torch.tensor(np.expand_dims(np.expand_dims(first_scans[2],0), -1)/255.0, dtype = torch.float)\nfixed_tensor = torch.tensor(np.expand_dims(np.expand_dims(first_scans[1],0), -1)/255.0, dtype = torch.float)","metadata":{"_uuid":"639867fb2f3e84334c3d884e3d1bde5ed9446a97","execution":{"iopub.status.busy":"2022-01-12T10:14:31.123818Z","iopub.execute_input":"2022-01-12T10:14:31.124313Z","iopub.status.idle":"2022-01-12T10:14:31.135628Z","shell.execute_reply.started":"2022-01-12T10:14:31.124260Z","shell.execute_reply":"2022-01-12T10:14:31.134825Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"torch_registered_scan_2 = model.forward(moving_tensor)\nt_img = 255.0*torch_registered_scan_2.detach().numpy()[0,:,:,0]","metadata":{"_uuid":"b5e143807ac00c25148a500b839b99a8531e6b26","execution":{"iopub.status.busy":"2022-01-12T10:14:32.009233Z","iopub.execute_input":"2022-01-12T10:14:32.009517Z","iopub.status.idle":"2022-01-12T10:14:32.046196Z","shell.execute_reply.started":"2022-01-12T10:14:32.009472Z","shell.execute_reply":"2022-01-12T10:14:32.045492Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax1a, ax4, ax5)) = plt.subplots(2, 3, figsize = (12, 8))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(first_scans[2], cmap = 'bone', vmax = 255)\nax2.set_title('Scan #2')\nax3.imshow(1.0*first_scans[2]-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference')\nax1a.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1a.set_title('Scan #1')\nax4.imshow(t_img, cmap = 'bone', vmax = 255)\nax4.set_title('Registered Scan 2')\nax5.imshow(t_img-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax5.set_title('Post Registration Difference');","metadata":{"_uuid":"2937f4ff654ada87a744896233684d522c8ec63e","execution":{"iopub.status.busy":"2022-01-12T10:14:34.749795Z","iopub.execute_input":"2022-01-12T10:14:34.750125Z","iopub.status.idle":"2022-01-12T10:14:35.580740Z","shell.execute_reply.started":"2022-01-12T10:14:34.750066Z","shell.execute_reply":"2022-01-12T10:14:35.579932Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=5e-3)\nfor epoch in range(100):\n    optimizer.zero_grad()\n    torch_registered_scan_2 = model.forward(moving_tensor)\n    loss = F.mse_loss(torch_registered_scan_2, fixed_tensor)\n    loss.backward()\n    optimizer.step()\n    if (epoch % 10)==0:\n        print('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch, loss.item()))\nprint('Final Parameters', list(model.parameters()))\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (12, 6))\nxx, yy = np.meshgrid(range(first_scans[1].shape[0]), range(first_scans[1].shape[1]))\ntest_pattern = (((xx % 40)>30)|((yy % 40)>30)).astype(np.float32)\nax1.imshow(test_pattern, cmap = 'bone_r')\nax1.set_title('Test Pattern')\ntest_tensor = torch.tensor(np.expand_dims(np.expand_dims(test_pattern, 0), -1), dtype = torch.float)\nskew_pattern = model.forward(test_tensor).detach().numpy()[0,:,:,0]\nax2.imshow(skew_pattern, cmap = 'bone_r')\nax2.set_title('Registered Pattern');","metadata":{"_uuid":"3ce7b620be8ba956366b090c79a2366432352f25","execution":{"iopub.status.busy":"2022-01-12T10:14:41.147706Z","iopub.execute_input":"2022-01-12T10:14:41.148047Z","iopub.status.idle":"2022-01-12T10:14:43.040133Z","shell.execute_reply.started":"2022-01-12T10:14:41.147998Z","shell.execute_reply":"2022-01-12T10:14:43.039243Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax1a, ax4, ax5)) = plt.subplots(2, 3, figsize = (12, 8))\nax1.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1.set_title('Scan #1')\nax2.imshow(first_scans[2], cmap = 'bone', vmax = 255)\nax2.set_title('Scan #2')\nax3.imshow(1.0*first_scans[2]-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax3.set_title('Difference')\nax1a.imshow(first_scans[1], cmap = 'bone', vmax = 255)\nax1a.set_title('Scan #1')\nt_img = 255.0*torch_registered_scan_2.detach().numpy()[0,:,:,0]\nax4.imshow(t_img, cmap = 'bone', vmax = 255)\nax4.set_title('Registered Scan 2')\nax5.imshow(t_img-first_scans[1], vmin = -100, vmax = 100, cmap = 'RdBu')\nax5.set_title('Post Registration Difference');","metadata":{"_uuid":"f3d0c18185dc3a8f096025876e34f71932df66dc","execution":{"iopub.status.busy":"2022-01-12T10:14:43.041357Z","iopub.execute_input":"2022-01-12T10:14:43.041867Z","iopub.status.idle":"2022-01-12T10:14:43.927354Z","shell.execute_reply.started":"2022-01-12T10:14:43.041810Z","shell.execute_reply":"2022-01-12T10:14:43.926503Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Tensorflow Approach\nHere we use tensorflow to approach the same problem. Tensorflow is much more flexible and allows us to try a number of different loss and registration approaches, but we have to code quite a bit by hand","metadata":{"_cell_guid":"fdd0a7a1-6a41-4e8b-9582-1435eebaddc0","_uuid":"1e21015e5e1d19dd44c08c1de1955fda8640b33e"}},{"cell_type":"code","source":"\"\"\"\nCode taken from https://github.com/kevinzakka/spatial-transformer-network/blob/master/transformer.py\n\"\"\"\ndef affine_transform(input_fmap, theta, out_dims=None, **kwargs):\n    \"\"\"\n    Spatial Transformer Network layer implementation as described in [1].\n    The layer is composed of 3 elements:\n    - localisation_net: takes the original image as input and outputs \n      the parameters of the affine transformation that should be applied\n      to the input image.\n    - affine_grid_generator: generates a grid of (x,y) coordinates that \n      correspond to a set of points where the input should be sampled \n      to produce the transformed output.\n    - bilinear_sampler: takes as input the original image and the grid\n      and produces the output image using bilinear interpolation.\n    Input\n    -----\n    - input_fmap: output of the previous layer. Can be input if spatial\n      transformer layer is at the beginning of architecture. Should be \n      a tensor of shape (B, H, W, C). \n    - theta: affine transform tensor of shape (B, 6). Permits cropping, \n      translation and isotropic scaling. Initialize to identity matrix. \n      It is the output of the localization network.\n    Returns\n    -------\n    - out_fmap: transformed input feature map. Tensor of size (B, H, W, C).\n    Notes\n    -----\n    [1]: 'Spatial Transformer Networks', Jaderberg et. al,\n         (https://arxiv.org/abs/1506.02025)\n    \"\"\"\n    # grab input dimensions\n    B = tf.shape(input_fmap)[0]\n    H = tf.shape(input_fmap)[1]\n    W = tf.shape(input_fmap)[2]\n    C = tf.shape(input_fmap)[3]\n\n    # reshape theta to (B, 2, 3)\n    theta = tf.reshape(theta, [B, 2, 3])\n\n    # generate grids of same size or upsample/downsample if specified\n    if out_dims:\n        out_H = out_dims[0]\n        out_W = out_dims[1]\n        batch_grids = affine_grid_generator(out_H, out_W, theta)\n    else:\n        batch_grids = affine_grid_generator(H, W, theta)\n\n    x_s = batch_grids[:, 0, :, :]\n    y_s = batch_grids[:, 1, :, :]\n\n    # sample input with grid to get output\n    out_fmap = bilinear_sampler(input_fmap, x_s, y_s)\n\n    return out_fmap\n\ndef get_pixel_value(img, x, y):\n    \"\"\"\n    Utility function to get pixel value for coordinate\n    vectors x and y from a  4D tensor image.\n    Input\n    -----\n    - img: tensor of shape (B, H, W, C)\n    - x: flattened tensor of shape (B*H*W, )\n    - y: flattened tensor of shape (B*H*W, )\n    Returns\n    -------\n    - output: tensor of shape (B, H, W, C)\n    \"\"\"\n    shape = tf.shape(x)\n    batch_size = shape[0]\n    height = shape[1]\n    width = shape[2]\n\n    batch_idx = tf.range(0, batch_size)\n    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n    b = tf.tile(batch_idx, (1, height, width))\n\n    indices = tf.stack([b, y, x], 3)\n\n    return tf.gather_nd(img, indices)\n\ndef affine_grid_generator(height, width, theta):\n    \"\"\"\n    This function returns a sampling grid, which when\n    used with the bilinear sampler on the input feature \n    map, will create an output feature map that is an \n    affine transformation [1] of the input feature map.\n    Input\n    -----\n    - height: desired height of grid/output. Used\n      to downsample or upsample. \n    - width: desired width of grid/output. Used\n      to downsample or upsample. \n    - theta: affine transform matrices of shape (num_batch, 2, 3). \n      For each image in the batch, we have 6 theta parameters of \n      the form (2x3) that define the affine transformation T.\n    Returns\n    -------\n    - normalized gird (-1, 1) of shape (num_batch, 2, H, W).\n      The 2nd dimension has 2 components: (x, y) which are the \n      sampling points of the original image for each point in the\n      target image.\n    Note\n    ----\n    [1]: the affine transformation allows cropping, translation, \n         and isotropic scaling.\n    \"\"\"\n    # grab batch size\n    num_batch = tf.shape(theta)[0]\n\n    # create normalized 2D grid\n    x = tf.linspace(-1.0, 1.0, width)\n    y = tf.linspace(-1.0, 1.0, height)\n    x_t, y_t = tf.meshgrid(x, y)\n\n    # flatten\n    x_t_flat = tf.reshape(x_t, [-1])\n    y_t_flat = tf.reshape(y_t, [-1])\n\n    # reshape to [x_t, y_t , 1] - (homogeneous form)\n    ones = tf.ones_like(x_t_flat)\n    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n\n    # repeat grid num_batch times\n    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n\n    # cast to float32 (required for matmul)\n    theta = tf.cast(theta, 'float32')\n    sampling_grid = tf.cast(sampling_grid, 'float32')\n\n    # transform the sampling grid - batch multiply\n    batch_grids = tf.matmul(theta, sampling_grid)\n    # batch grid has shape (num_batch, 2, H*W)\n\n    # reshape to (num_batch, H, W, 2)\n    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n\n    return batch_grids\n\ndef bilinear_sampler(img, x, y):\n    \"\"\"\n    Performs bilinear sampling of the input images according to the \n    normalized coordinates provided by the sampling grid. Note that \n    the sampling is done identically for each channel of the input.\n    To test if the function works properly, output image should be\n    identical to input image when theta is initialized to identity\n    transform.\n    Input\n    -----\n    - img: batch of images in (B, H, W, C) layout.\n    - grid: x, y which is the output of affine_grid_generator.\n    Returns\n    -------\n    - interpolated images according to grids. Same size as grid.\n    \"\"\"\n    # prepare useful params\n    B = tf.shape(img)[0]\n    H = tf.shape(img)[1]\n    W = tf.shape(img)[2]\n    C = tf.shape(img)[3]\n\n    max_y = tf.cast(H - 1, 'int32')\n    max_x = tf.cast(W - 1, 'int32')\n    zero = tf.zeros([], dtype='int32')\n\n    # cast indices as float32 (for rescaling)\n    x = tf.cast(x, 'float32')\n    y = tf.cast(y, 'float32')\n\n    # rescale x and y to [0, W/H]\n    x = 0.5 * ((x + 1.0) * tf.cast(W, 'float32'))\n    y = 0.5 * ((y + 1.0) * tf.cast(H, 'float32'))\n\n    # grab 4 nearest corner points for each (x_i, y_i)\n    # i.e. we need a rectangle around the point of interest\n    x0 = tf.cast(tf.floor(x), 'int32')\n    x1 = x0 + 1\n    y0 = tf.cast(tf.floor(y), 'int32')\n    y1 = y0 + 1\n\n    # clip to range [0, H/W] to not violate img boundaries\n    x0 = tf.clip_by_value(x0, zero, max_x)\n    x1 = tf.clip_by_value(x1, zero, max_x)\n    y0 = tf.clip_by_value(y0, zero, max_y)\n    y1 = tf.clip_by_value(y1, zero, max_y)\n\n    # get pixel value at corner coords\n    Ia = get_pixel_value(img, x0, y0)\n    Ib = get_pixel_value(img, x0, y1)\n    Ic = get_pixel_value(img, x1, y0)\n    Id = get_pixel_value(img, x1, y1)\n    \n    # recast as float for delta calculation\n    x0 = tf.cast(x0, 'float32')\n    x1 = tf.cast(x1, 'float32')\n    y0 = tf.cast(y0, 'float32')\n    y1 = tf.cast(y1, 'float32')\n\n    # calculate deltas\n    wa = (x1-x) * (y1-y)\n    wb = (x1-x) * (y-y0)\n    wc = (x-x0) * (y1-y)\n    wd = (x-x0) * (y-y0)\n\n    # add dimension for addition\n    wa = tf.expand_dims(wa, axis=3)\n    wb = tf.expand_dims(wb, axis=3)\n    wc = tf.expand_dims(wc, axis=3)\n    wd = tf.expand_dims(wd, axis=3)\n\n    # compute output\n    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n\n    return out","metadata":{"_cell_guid":"610da929-2b2c-41af-9926-6cff9ab020ec","_uuid":"7d118fc1287f1277c12b41c120257a3ca1c8b214","execution":{"iopub.status.busy":"2022-01-12T10:15:10.143277Z","iopub.execute_input":"2022-01-12T10:15:10.143614Z","iopub.status.idle":"2022-01-12T10:15:11.000154Z","shell.execute_reply.started":"2022-01-12T10:15:10.143552Z","shell.execute_reply":"2022-01-12T10:15:10.999031Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"g = tf.Graph()\nwith g.as_default():\n    init = tf.global_variables_initializer()\n    # tf Graph Input\n    fixed_img = tf.placeholder(\"float\", shape = (1, None, None, 1), name = 'FixedImage')\n    moving_img = tf.placeholder(\"float\", shape = (1, None, None, 1), name = 'MovingImage')\n    # Initialize the variables (i.e. assign their default value)\n    \n    with tf.name_scope('transform_parameters'): # Set transform parameters\n        x_offset = tf.Variable(0.0, name=\"x_offset\")\n        y_offset = tf.Variable(0.0, name=\"y_offset\")\n        # we keep scale and rotation fixed\n        scale = tf.placeholder(\"float\", shape=tuple(), name = \"scale\")\n        rotation = tf.placeholder(\"float\", shape = tuple(), name = \"rotation\")\n\n    with tf.name_scope('transformer_and_interpolator'):\n        flat_mat = tf.tile([tf.cos(rotation), -tf.sin(rotation), x_offset, \n                            tf.sin(rotation), tf.cos(rotation), y_offset], (1,))\n        flat_mat = tf.reshape(flat_mat, (1, 6))\n        trans_tensor = affine_transform(moving_img, flat_mat)\n\n    with tf.name_scope('metric'):\n        mse = tf.reduce_mean(tf.square(fixed_img-trans_tensor), name = 'MeanSquareError')\n        optimizer = tf.train.GradientDescentOptimizer(5e-6).minimize(mse)","metadata":{"_cell_guid":"3847637f-7037-4ed5-8031-37aa4c3e01e6","_uuid":"4127b6117c2f770526a9d405c7bac0059bdd450e","execution":{"iopub.status.busy":"2022-01-12T10:15:11.097139Z","iopub.execute_input":"2022-01-12T10:15:11.097579Z","iopub.status.idle":"2022-01-12T10:15:11.691129Z","shell.execute_reply.started":"2022-01-12T10:15:11.097501Z","shell.execute_reply":"2022-01-12T10:15:11.690275Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Start training\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import Image\nimport matplotlib.animation as animation\nWriter = animation.writers['imagemagick']\nwriter = Writer(fps=5, metadata=dict(artist='Me'), bitrate=1800)\ndef make_feed_dict(f_img, m_img):\n    return {fixed_img: np.expand_dims(np.expand_dims(f_img,0), -1),\n            moving_img: np.expand_dims(np.expand_dims(m_img,0), -1),\n            rotation: 0.0}\nloss_history = []\noptimize_iters = 30\nwith tf.Session(graph = g) as sess:\n    plt.close('all')\n    fig, m_axs = plt.subplots(2, 2, figsize=(12, 12), dpi=120)\n    tf.initialize_all_variables().run()\n    # Run the initializer\n    sess.run(init)\n    # Fit all training data\n    const_feed_dict = make_feed_dict(first_scans[1], first_scans[2])\n    def update_frame(i):\n        global loss_history\n        (ax1, ax2), (ax4, ax3) = m_axs\n        for c_ax in m_axs.flatten():\n            c_ax.cla()\n            c_ax.axis('off')\n        f_mse, x_pos, y_pos, rs_img = sess.run([mse, x_offset, y_offset, trans_tensor], \n                                                      feed_dict=const_feed_dict)\n        loss_history += [f_mse]\n        \n        ax1.imshow(first_scans[1], cmap='bone')\n        ax1.set_title('$T_0$')\n        ax2.imshow(first_scans[2], cmap='bone')\n        ax2.set_title('$T_1$')\n        #ax3.imshow(rs_img[0,:,:,0], cmap = 'bone')\n        #ax3.set_title('Output')\n        ax4.imshow(first_scans[1]*1.0-rs_img[0,:,:,0], cmap = 'RdBu', vmin = -100, vmax = 100)\n        ax4.set_title('Difference\\nMSE: %2.2f' % (f_mse))\n        ax3.semilogy(loss_history)\n        ax3.set_xlabel('Iteration')\n        ax3.set_ylabel('MSE (Log-scale)')\n        ax3.axis('on')\n        \n        for _ in range(10):\n            sess.run(optimizer, feed_dict=const_feed_dict)\n    if False: # kaggle is missing ffmpeg and the gif way doesn't work great yet\n        # write animation frames\n        anim_code = FuncAnimation(fig,\n                              update_frame,\n                              frames=optimize_iters,\n                              interval=1000,\n                              repeat_delay=2000)\n        anim_code.save('tf_registration.gif', writer=writer)\n    else:\n        for i in range(optimize_iters):\n            update_frame(i)\n","metadata":{"_cell_guid":"72a9ac81-fc25-43b6-9f20-73b870be2bc8","_uuid":"3c6d07840394410af2340b7f68ea9627a21c546c","execution":{"iopub.status.busy":"2022-01-12T10:15:12.145431Z","iopub.execute_input":"2022-01-12T10:15:12.145719Z","iopub.status.idle":"2022-01-12T10:15:40.169441Z","shell.execute_reply.started":"2022-01-12T10:15:12.145679Z","shell.execute_reply":"2022-01-12T10:15:40.168594Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"5a9ffdb6-f388-4818-afd5-20b8763c3ba7","_uuid":"0e578153d7b166e2786750510d8bf89eba3ae220","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}